// Configuration for Load Sharing Facility (LSF) workload management platform

executor {
    name = 'lsf'

    // Set total number of jobs that can be simultaneously run
    queueSize = 1000
    killBatchSize = 1000

    // Set perJobMemLimit to true. See:
    // * https://github.com/nextflow-io/nextflow/issues/123
    // * https://gitter.im/nextflow-io/nextflow/archives/2018/02/09
    perJobMemLimit = true
} // end executor

conda {
    cacheDir = "${projectDir}/../nf_cacheDir"
}

singularity {
    enabled     = true
    autoMounts  = true
    // USER could set this via NXF_SINGULARITY_CACHEDIR
    // make sure images are available, may need to copy from /software/team152/nextflow/cache_singularity
    cacheDir = '/lustre/scratch123/hgi/projects/ukbb_scrna/pipelines/singularity_images'
    runOptions = '--containall --dns 172.18.255.1,172.18.255.2,172.18.255.3 --bind /lustre'

    envWhitelist = 'HOSTNAME,SSH_CONNECTION,SSH_CLIENT,CVS_RSH,http_proxy,https_proxy,HTTP_PROXY'
}

process {
    executor = 'lsf'

    // lsf users will need to edit the below parameters to fit their platform
    queue = 'normal'
    // native configuration options
    //clusterOptions = { "-R \"select[mem>${task.memory.toMega()}]\"" }
    //clusterOptions = { "-R \"span[hosts=1]\"" }

    // specific settings for processes with specific labels such as
    // big_mem, short, long
    //withLabel: big_mem {
    //    cpus = 16
    //    memory = 64.GB
    //    queue = 'hugemem'
    //}
    withLabel: long_job {
        queue = 'long'
    }
    withLabel: gpu {
        cpus = 1
        // maxForks = 1
        // queue = 'gpu-normal'
        queue = 'gpu-huge'
        clusterOptions = { "-gpu \"num=1\"" }
        containerOptions = {
            workflow.containerEngine == "singularity" ? '--nv':
            ( workflow.containerEngine == "docker" ? '--gpus all': null )
        }
    }
} // end process
