// Configuration for Load Sharing Facility (LSF) workload management platform

executor {
    name = 'lsf'

    // Set total number of jobs that can be simultaneously run
    queueSize = 1000
    killBatchSize = 1000
    
    // Set perJobMemLimit to true. See:
    // * https://github.com/nextflow-io/nextflow/issues/123
    // * https://gitter.im/nextflow-io/nextflow/archives/2018/02/09
    perJobMemLimit = true
} // end executor

singularity {
    enabled     = true
    autoMounts  = true
    // USER should set this via NXF_SINGULARITY_CACHEDIR
    // cacheDir = '/lustre/scratch118/humgen/resources/containers/'
    runOptions = '--dns 172.18.255.1,172.18.255.2,172.18.255.3'
    envWhitelist = 'HOSTNAME,SSH_CONNECTION,SSH_CLIENT,CVS_RSH,http_proxy,https_proxy,HTTP_PROXY'
}

process {
    executor = 'lsf'

    // lsf users will need to edit the below parameters to fit their platform
    queue = 'normal'
    // native configuration options
    //clusterOptions = { "-R \"select[mem>${task.memory.toMega()}]\"" }
    //clusterOptions = { "-R \"span[hosts=1]\"" }

    // specific settings for processes with specific labels such as
    // big_mem, short, long
    //withLabel: big_mem {
    //    cpus = 16
    //    memory = 64.GB
    //    queue = 'hugemem'
    //}
    withLabel: long_job {
        queue = 'long'
    }
    withLabel: gpu {
        cpus = 1
        // maxForks = 1
        // queue = 'gpu-normal'
        queue = 'gpu-huge'
        clusterOptions = { "-gpu \"num=1\"" }
        containerOptions = {
            workflow.containerEngine == "singularity" ? '--nv':
            ( workflow.containerEngine == "docker" ? '--gpus all': null )
        }
    }
    // Tensorflow wants to use all available memory on a GPU, so make sure we
    // request lots of memory. Most nodes have 754.5G on Sanger farm, so
    // request ~1/2.
    withName: cluster_validate_resolution_keras {
        memory = 370.GB
        //memory = 150.GB
        cpus = 1
    }
} // end process
